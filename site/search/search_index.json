{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Latest Posts","text":"A beamer template for academic presentations The Covariance Matrix Adaptation Evolution Strategy Selectively merging files across branches Apple Self-hosted GitHub Runner"},{"location":"about/","title":"None","text":"<p>This is a site to store the ongoing processes and workflows that I learn throughout the day. This may be useful for others working in a similar field, or at least entertaining with a twist of usefulness for everyone else.</p>"},{"location":"about/#whoami","title":"<code>&gt;&gt; whoami</code>","text":"<p>Dr Brady Planden is a Postdoctoral Research Associate at the University of Oxford. He received his Ph.D. from Oxford Brookes University with a thesis entitled \"Improvements on physics-informed models for lithium batteries\", which focused on reduced order electrochemical modelling techniques. Brady completed his Bachelor of Engineering at the University of Victoria. Brady is currently doing the following,</p> <ul> <li>Developing software for electrochemical modelling (PyBOP, LiiBRA, Galv)</li> <li>Conducting research on new lithium-ion battery chemistries with the EU's IntelLiGent Consortium</li> <li>Works as a post-doc in the Battery Intelligence Lab. This includes mentoring, random research IT, and a small amount of lecturing, etc.</li> </ul>"},{"location":"about/#ls","title":"<code>&gt;&gt; ls</code>","text":"<p>Here's a few resources that I have found useful (in no particular order):</p>  Writing   Dev   Research   Random  <ul> <li> DeepL Write  </li> <li> WizardLM  </li> <li> Claude AI  </li> <li> MacTex  </li> </ul> <li> Codecov  </li> <li> Pull Requests  </li> <li> Oxford RSE  </li> <li> tailscale  </li> <li> kitty terminal  </li> <li> iTerm2  </li> <li> OhMyZSH  </li> <li> Oh Shit, Git!?! </li> <li> Juliaup  </li> <li> pyenv  </li> <li> Advanced Python Mastery  </li> <li> Academic template  </li> <li> Simo S\u00e4rkk\u00e4  </li> <li> ungoogled chrome  </li> <li>  solaar  </li> <li> Remmina  </li> <li> Timeshift  </li> <li> backintime  </li> <li> bitwarden  </li> <li> fzf  </li>"},{"location":"about/#git-blame","title":"<code>&gt;&gt; git blame</code>","text":"<p>This site is built with Material for MkDocs, and was inspired from \"Why, Scott, Why?!\".</p>"},{"location":"publications/","title":"None","text":""},{"location":"publications/#google-scholar-profile","title":"Google Scholar Profile","text":"<p>Looping in the Human: Collaborative and Explainable Bayesian Optimization  M Adachi, B Planden, DA Howey, K Maundet, MA Osborne, SL Chau. 2023.  [Arxiv]</p> <p>Investigation of constant stack pressure on lithium-ion battery performance  A Leonard, B Planden, K Lukow, D Morrey. 2023.  [DOI] [pdf]</p> <p>A computationally informed realisation algorithm for lithium-ion batteries implemented with LiiBRA.jl  B Planden, K Lukow, P Henshall, G Collier, D Morrey. 2022.  [Arxiv] [DOI]</p> <p>BattPhase\u2014A Convergent, Non-Oscillatory, Efficient Algorithm and Code for Predicting Shape Changes in Lithium Metal Batteries Using Phase-Field Models: Part I. Secondary Current Distribution  T Jang, L Mishra, S A Roberts, B Planden, A Subramaniam, M Uppaluri, D Linder, M P Gururajan, J Zhang, V R Subramanian. 2022.  [Pre-Print] [DOI]</p> <p>Improvements on physics-informed models for lithium batteries  B Planden. 2022.  [Thesis]</p>"},{"location":"blog/2023/beamer/","title":"Beamer Template","text":"<p>In an effort to move away from PowerPoint, I've been looking for an alternative presentation software. I've been using LaTeX for over ten years now and have found it to be a great tool for writing reports and papers. So, I thought I would give Beamer a try (and it's been great!).</p> <p>I found a great starting template from the University of Oxford Maths' template. Starting from this template, I've updated it to a modern 16:9 aspect ratio and styling for my current lab, the Battery Intelligence Lab.</p> <p>The template is available on GitHub, with a sample presentation below.</p> <p></p>"},{"location":"blog/2023/cma-es/","title":"The Covariance Matrix Adaptation Evolution Strategy (CMA-ES)","text":"<p>Recently I've been working on improved parameterisation and optimisation methods for continuum scale battery models. One of the challenges in predictive modelling is parameter identification from measured data. In the electrochemical field, this data is often time-series in nature, collected from potentiostatic and/or galvanostatic experiments on different time scales.</p> <p>This challenge becomes even more difficult when the model structure is overly constrained (e.g. multi-particle based continuum models) and there is a lack of information for unique identification. To overcome this, I have been looking for improved optimisation methods to identify the model parameters, and one such method has caught my interest and I thought I would discuss it here.</p>"},{"location":"blog/2023/cma-es/#what-is-cma-es","title":"What is CMA-ES?","text":"<p>CMA-ES is a stochastic optimisation algorithm that provides a global search for the functional critical point. It is a derivative-free algorithm that uses a population of candidate solutions to iteratively update the search distribution. This method is particularly useful for non-linear, non-convex and multimodal optimisation problems. </p>"},{"location":"blog/2023/cma-es/#step-1-sampling-evaluation","title":"Step 1 - Sampling &amp; Evaluation","text":"<p>The algorithm starts by sampling candidate solutions from a multivariate normal distribution at step \\((g+1)\\). The candidates are then evaluated and stored in ascending order of performance, i.e. element zero stores the current best candidate from the optimiser.</p> \\[     \\mathbf{x}_{k}^{(g+1)} \\sim \\mathbf{m}^{(g)} + \\sigma^{(g)} \\mathcal{N}\\left(0, \\mathbf{C}^{(g)}\\right), \\ \\ \\ \\ \\ \\ \\text{for} \\ g = 1,...,\\lambda \\] <p>where,  \\(\\mathbf{x}_{k}^{(g+1)}\\) is the \\(k^\\text{th}\\) candidate at the \\((g+1)\\) iteration  \\(\\mathbf{m}^{(g)}\\) is the mean vector of the search distribution at \\(g\\) \\(\\sigma^{(g)}\\) is the step size at \\(g\\) \\(\\mathbf{C}^{(g)}\\) is the covariance matrix.  \\(\\mathcal{N}\\left(0, \\mathbf{C}^{(g)}\\right)\\) is a multivariate normal distribution with corresponding zero mean and covariance matrix \\(\\mathbf{C}^{(g)}\\) \\(\\lambda \\geq 2\\), is the population size </p>"},{"location":"blog/2023/cma-es/#step-2-construct-the-evolution-paths","title":"Step 2 - Construct the Evolution Paths","text":"<p>Using a weight function \\(w_i\\) and a subset of the population, known as the parent value, the weighted averages as \\(dy = \\sum_{i=1}^{\\mu} w_i y_{1:\\lambda}\\) and \\(dz = \\sum_{i=1}^{\\mu} w_i z_{1:\\lambda}\\) are computed. The evolution path at generation \\(g\\) is then constructed as,</p> \\[     \\mathbf{p}_{\\text{c}}^{(g+1)} = (1 - c_{\\text{c}}) \\mathbf{p}_{\\text{c}}^{(g)} + h_{\\sigma}^{(g+1)} \\sqrt{c_{\\text{c}}(2 - c_{\\text{c}})\\mu_w dy} \\] <p>where the conjugate evolution path is then constructed as,</p> \\[     \\mathbf{p}_{\\sigma}^{(g+1)} = (1 - c_\\sigma) \\mathbf{p}_{\\sigma}^{(g)} + \\sqrt{c_\\sigma(2 - c_\\sigma)\\mu_w dz} \\] <p>where, \\(\\mu_w = 1/\\sum_{i=1}^\\mu w_i^2\\), and \\(c_{\\text{c}}\\), \\(c_{\\sigma}\\) are the cumulation factors, with \\(h_{\\sigma}^{(g+1)}\\) is the Heaviside function, defined as,</p> \\[ h_{\\sigma}^{(g+1)} = \\begin{cases}     1, &amp; \\text{if } \\frac{\\left\\lVert \\mathbf{p}_{\\sigma}^{(g+1)} \\right\\rVert^2}{1-(1-c_\\sigma)^{2\\cdot(g+1)}} &lt; \\left(2+4/(d+1)\\right)d, \\\\     0, &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"blog/2023/cma-es/#step-3-update-the-search-distribution","title":"Step 3 - Update the Search Distribution","text":"<p>The algorithm updates the mean search distribution via</p> \\[     \\mathbf{m}^{(g+1)} = \\mathbf{m}^{(g)} + c_\\text{m}\\sum_{i=1}^{\\mu} w_{i} \\left(\\mathbf{x}_{i:\\lambda}^{(g+1)} - \\mathbf{m}^{(g)}\\right) \\] <p>where,  \\(w_i\\) is a positive weighting vector  \\(\\mu \\leq \\lambda\\) is the number of solutions used to update the mean vector  \\(c_\\text{m}\\) is the learning rate </p> <p>Essentially, the mean from generation \\((g)\\) is summed with a weighted sum of the geometric difference between the \\((g+1)\\) samples multiplied by the learning rate. To update the step size, the algorithm uses the following update rule,</p> \\[     \\sigma^{(g+1)} = \\sigma^{(g)} \\exp\\left(\\frac{c_\\sigma}{d_\\sigma}\\left(\\frac{\\left\\lVert \\mathbf{p}_{\\sigma}^{(g+1)} \\right\\rVert}{\\mathbb{E}\\left\\lVert \\mathcal{N}\\left(0, \\mathbf{I}\\right) \\right\\rVert} - 1\\right)\\right) \\] <p>where \\(d_\\sigma\\) is a damping parameter on \\(\\sigma^g\\). Next, to estimate the covariance matrix,</p> \\[     \\mathbf{C}^{(g+1)} =  \\left(1-c_1-c_\\mu \\sum w_j\\right) \\mathbf{C}^{(g)} + c_1 \\underbrace{\\mathbf{p}_{\\text{c}}^{(g+1)} \\mathbf{p}_{\\text{c}}^{(g+1)^T}}_{\\text{rank-one update}} + c_\\mu \\underbrace{\\sum_{i=1}^{\\lambda} w_{i} \\mathbf{y}_{i:\\lambda}^{(g+1)} \\left(\\mathbf{y}_{i:\\lambda}^{(g+1)}\\right)^T}_{\\text{rank-}\\mu \\ \\text{update}} \\] <p>where,  \\(c_1 \\approx 2/n^2\\),  \\(c_\\mu \\approx \\text{min}(\\mu_{\\text{eff}}/n^2, 1-c_1)\\) \\(y_{i:\\lambda}^{(g+1)} = \\left(\\mathbf{x}_{i:\\lambda}^{(g+1)} - \\mathbf{m}^{(g)}\\right)/\\sigma^{(g)}\\) </p> <p>This algorithm is repeated until the termination criteria is met with the best candidate stored on each iteration. The full algorithm is given in [1,2].</p>"},{"location":"blog/2023/cma-es/#example","title":"Example","text":"<p>Let's look at how CMA-ES deals with the minimisation of a simple two-dimensional parabola function distorted by Gaussian noise. First, the underlying function is given as \\(f(\\mathbf{x}) = \\mathbf{x}^2\\), where \\(\\mathbf{x} \\in \\mathbb{R}^2\\). A Julia script to construct this function is given below,</p> <pre><code>using GLMakie, Distributions, Dierckx\n\nxy = range(-10, stop = 10, length = 100)\nz = zeros(length(xy)+1,length(xy)+1)\n\nf(x, y) = x^2 + y^2\nz = [f(x, y) for x in xy, y in xy]\n</code></pre> <p>This gives us the following surface,</p> <p>A nice parabolic function with a global minimum at \\((0,0)\\). Now let's add some noise to the function, we do this with the Distributions.jl package in Julia. Let's sample from a normal distribution with a mean of zero and a standard deviation of 5. The updated function is given by \\(f(\\mathbf{x}) = \\mathbf{x}^2 + \\varepsilon\\), where \\(\\varepsilon \\sim \\mathcal{N}(0, 5)\\). The updated Julia form is then</p> <pre><code>using GLMakie, Distributions, Dierckx\n\nsigma = 5\nxy = range(-10, stop = 10, length = 100)\nz = zeros(length(xy)+1,length(xy)+1)\nnoise = rand(Normal(0, sigma), length(xy) * length(xy))\n\u03bc = Spline2D(xy,xy,reshape(noise,100,100))\n\nf(x, y) = x^2 + y^2 + \u03bc(x,y)\nz = [f(x, y) for x in xy, y in xy]\n</code></pre> <p>Due to the high noise covariance, exploring the functional surface is quite challenging as the gradient information is heavily corrupted by the Gaussian noise. Since the CMA-ES algorithm doesn't need the gradient information, it is a good candidate for this problem. To implement the CMA-ES algorithm, we will use the Evolutionary.jl package. To provide a reference for the performance of CMA-ES on this problem, I've added a gradient descent method using Optim.jl. This is implemented with the CMA-ES below.</p> <pre><code>using Evolutionary, Distributions, Optim\n\n# Update functional form and optimize\nx0 = [-12.4,9.53]\nf(x) = x[1]^2 + x[2]^2 + \u03bc(x[1],x[2])\nres_cmaes = Evolutionary.optimize(f,x0, CMAES(sigma0=1), Evolutionary.Options(store_trace=true))\nres_grad = Optim.optimize(f, x0, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n</code></pre> <p>The results of each of the optimisation algorithms constructed above are shown on the noisy functional surface given below,</p> <p>As expected, the gradient descent method struggles with the large number of local optima created by the Gaussian noise, but the CMA-ES method is able to find the global minimum repeatedly. The code to repeat this example can be found in the here repository. That's it for this post, I hope you found it useful! Overall, CMA-ES provides a robust method for minimising a difficult cost function, but this robustness is traded off against performance.  If you have any questions or comments, please feel free to contact me.</p>"},{"location":"blog/2023/cma-es/#references","title":"References","text":"<p>[1] N. Hansen, \u2018The CMA Evolution Strategy: A Tutorial\u2019. arXiv, Mar. 10, 2023. Available: http://arxiv.org/abs/1604.00772  [2] M. Nomura, Y. Akimoto, and I. Ono, \u2018CMA-ES with Learning Rate Adaptation: Can CMA-ES with Default Population Size Solve Multimodal and Noisy Problems?\u2019, in Proceedings of the Genetic and Evolutionary Computation Conference, Jul. 2023, pp. 839\u2013847. doi: 10.1145/3583131.3590358.</p>"},{"location":"blog/2023/git-selective-merge/","title":"Selectively merging files from across branches","text":"<p>Recently, I've had the need to merge selective files from one branch into another. This isn't something I recommend doing often, but if you're stuck in a position where you need a few files from another branch, here is one way to do it:</p>"},{"location":"blog/2023/git-selective-merge/#the-process","title":"The process:","text":"<p>Let's say you have a branch named <code>feature-1</code> and want the file <code>example.py</code> from the <code>feature-2</code> branch:</p> <ol> <li>First, from the <code>feature-1</code> branch, merge the <code>feature-2</code> branch with squash option (<code>git merge --squash feature-2</code>) to avoid creating a merge commit.</li> <li>Reset the files (or entire folders) that you don't want in <code>feature-1</code> (<code>git reset origin/feature-1 tests/</code> or <code>git reset origin/feature-1 tests/test.py</code>) or remove them completely (<code>git rm tests/test.py</code>).</li> <li>Add and commit the file(s) you want in your branch with a new commit message (<code>git add example.py &amp;&amp; git commit -m \"merge example.py from feature-2\"</code>).</li> <li>Clean the branch to remove untracked changes (<code>git clean -xfd</code>), -f for force, -d to remove the untracked directories, and -x to remove the ignored files. </li> <li>Restore the tracked changes (<code>git restore .</code>)</li> <li>Push the changes to the remote branch (<code>git push</code>).</li> </ol>"},{"location":"blog/2023/github-runner/","title":"Apple Self-hosted GitHub Runner","text":"<p>In this post, I take a look at the process of creating a self-hosted ARM-based GitHub runner for CI/CD. This runner is currently being used for the development of PyBaMM and PyBOP.</p> <p>TL;DR: Configure a self-hosted Apple M2 GitHub runner to validate the deployment of your repositories on Apple-based ARM hardware. An example workflow is presented from PyBaMM's daily testing workflow. This example is over 2X faster than the GitHub-hosted hardware.</p>"},{"location":"blog/2023/github-runner/#what-is-a-runner","title":"What is a runner?","text":"<p>As per GitHub's definition:</p> <p>A self-hosted runner is a system that you deploy and manage to execute jobs from GitHub Actions</p> <p>So we are responsible for the \"system\" that will execute the GitHub actions. Seems reasonable, but why bother with this overhead when GitHub will manage the whole workflow for you? </p> <p>This is the crux of the TIL: to achieve both robust and high performance testing workflows, a self-hosted runner is a great solution (assuming you have the hardware lying around). </p> <p>As this article was being written, GitHub [https://github.blog/2023-10-02-introducing-the-new-apple-silicon-powered-m1-macos-larger-runner-for-github-actions/){:target='_blank'} released their Apple M1 runner. Previously, M-Series ARM support wasn't offered, so a self-hosted runner was needed to validate and develop on this hardware. Other potential use cases include the need to deploy private or proprietary software, or the need for a system environment that GitHub doesn't offer. This is also covered on GitHub,</p> <p>Self-hosted runners offer more control of hardware, operating system, and software tools than GitHub-hosted runners provide. With self-hosted runners, you can create custom hardware configurations that meet your needs with processing power or memory to run larger jobs, install software available on your local network, and choose an operating system not offered by GitHub-hosted runners. Self-hosted runners can be physical, virtual, in a container, on-premises, or in a cloud.</p>"},{"location":"blog/2023/github-runner/#lets-get-to-it","title":"Let's get to it","text":"<p>The setup begins with the creation of a runner-specific macOS account, such as <code>runner</code>, with limited privileges. This is the first step in hardening the runner against malicious code that might be deployed on it. To do this, run the following bash command on an account with administrative privileges.</p> <pre><code>account create runner\n</code></pre> <p>Next, log in to this newly created account and run the GitHub repository-specific commands to associate the runner with the selected repository. These can be found in the repository settings under <code>Settings -&gt; Actions -&gt; Runners</code>. This will look like:</p> <p></p> <p>The installation and validation commands then look like,</p> <p><pre><code># Create a folder\n$ mkdir actions-runner &amp;&amp; cd actions-runner\n# Download the latest runner package\n$ curl -o actions-runner-osx-arm64-2.308.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-osx-arm64-2.308.0.tar.gz\n# Optional: Validate the hash\n$ echo \"a8b2c25868e4296cbd203342754223dd2cc17f91585592c99ccd85b587d05310  actions-runner-osx-arm64-2.308.0.tar.gz\" | shasum -a 256 -c\n# Extract the installer\n$ tar xzf ./actions-runner-osx-arm64-2.308.0.tar.gz\n</code></pre> with the repository-specific configuration as,</p> <pre><code># Create the runner and start the configuration experience\n$ ./config.sh --url https://github.com/xxx/xxx --token xxxxxx\n# Last step, run it!\n$ ./run.sh\n</code></pre> <p>Running the above code will set up the runner configuration and links it to the repository of your choice. The runner should then appear in the GitHub repository as,</p> <p></p> <p>This is mostly it, the remaining setup involves adding the runner to your CI/CD workflows. This is fairly straightforward and requires the target workflow to use the tags associated with this runner. In this case, using <code>self-hosted</code>, <code>macOS</code> or <code>ARM64</code> will deploy the workflow on the self-hosted runner.</p>"},{"location":"blog/2023/github-runner/#example-workflow","title":"Example workflow","text":"<p>For an example of how to integrate your workflow onto the runner, the following is an excerpt from the PyBaMM scheduled test workflow.</p> <pre><code>#M-series Mac Mini\n  build-apple-mseries:\n    needs: style\n    runs-on: [self-hosted, macOS, ARM64]\n    env:\n      GITHUB_PATH: ${PYENV_ROOT/bin:$PATH}\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install python &amp; create virtualenv\n        shell: bash\n        run: |\n          eval \"$(pyenv init -)\"\n          pyenv install ${{ matrix.python-version }} -s\n          pyenv virtualenv ${{ matrix.python-version }} pybamm-${{ matrix.python-version }}\n\n      - name: Install dependencies &amp; run unit tests for Windows and MacOS\n        shell: bash\n        run: |\n          eval \"$(pyenv init -)\"\n          pyenv activate pybamm-${{ matrix.python-version }}\n          python -m pip install --upgrade pip wheel setuptools nox\n          python -m nox -s unit\n\n      - name: Run integration tests for Windows and MacOS\n        run: |\n          eval \"$(pyenv init -)\"\n          pyenv activate pybamm-${{ matrix.python-version }}\n          python -m nox -s integration\n\n      - name: Uninstall pyenv-virtualenv &amp; python\n        if: always()\n        shell: bash\n        run: |\n          eval \"$(pyenv init -)\"\n          pyenv activate pybamm-${{ matrix.python-version }}\n          pyenv uninstall -f $( python --version )\n</code></pre> <p>The above workflow produces the following:</p> <ul> <li>Sets the job to deploy on the self-hosted runner</li> <li>Installs Python and creates a virtual environment using pyenv</li> <li>Installs the required Python dependencies for PyBaMM and the test suite</li> <li>Runs the standard unit and integration tests using Nox</li> <li>Tears down the test environment (regardless of the test results)</li> </ul> <p>The results of this workflow are shown in the figure below. At the time of writing, the GitHub runner finishes in about 20 minutes, while the self-hosted runner finishes in 8 minutes.</p> <p></p>"}]}